#!/usr/bin/env python

"""
Project managements and Sample loop.
"""

from argparse import ArgumentParser
import os
import sys
import logging
from pipelines import Project
from pipelines import toolkit as tk
import cPickle as pickle
import pandas as pd
import time
import re
import textwrap
import shutil
import subprocess

# TODO: solve pandas chained assignments
pd.options.mode.chained_assignment = None

__author__ = "Andre Rendeiro"
__copyright__ = "Copyright 2015, Andre Rendeiro"
__credits__ = []
__license__ = "GPL2"
__version__ = "0.1"
__maintainer__ = "Andre Rendeiro"
__email__ = "arendeiro@cemm.oeaw.ac.at"
__status__ = "Development"


def main():
    # Parse command-line arguments
    parser = ArgumentParser(description="Project managements and Sample loop.")

    # Subparsers
    subparser = parser.add_subparsers(title="sub-command", dest="command")
    parser = mainArgParser(parser)
    subparser = preprocessArgParser(subparser)
    subparser = analyzeArgParser(subparser)
    subparser = compareArgParser(subparser)

    # Parse
    args = parser.parse_args()

    # Start project
    prj = Project(args.project_name)
    prj.addSampleSheet(args.csv)

    # Logging
    createLogger(args, prj)

    # Start main function
    if args.command in ["preprocess", "analyse"]:
        sampleLoop(args, prj)
    elif args.command == "stats":
        readStats(args, prj)
    elif args.command == "compare":
        compare(args, prj)

    # Exit
    prj.logger.info("Finished and exiting.")

    # Copy log to projectDir
    shutil.copy2(
        os.path.join(os.getcwd(), prj.name + ".log"),
        os.path.join(prj.dirs.runs, prj.name + ".log")
    )

    sys.exit(0)


def mainArgParser(parser):
    """
    Global options for pipeline.
    """
    # Project
    parser.add_argument(dest="project_name", help="Project name.", type=str)
    parser.add_argument(dest="csv", help="CSV file with sample annotation.", type=str)  # improvement: check project dirs for csv

    # Behaviour
    parser.add_argument("-k", "--keep-tmp", dest="keep_tmp", action="store_true",
                        help="Keep intermediary files. If not it will only preserve final files. Default=False.")
    parser.add_argument("-l", "--log-level", default="INFO", dest="log_level",
                        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                        help="Logging level. Default=INFO.", type=str)
    parser.add_argument("--dry-run", dest="dry_run", action="store_true",
                        help="Dry run. Assemble commands, but do not submit jobs to slurm. Default=False.")
    parser.add_argument("--no-checks", dest="checks", action="store_false",
                        help="Don't check file existence and integrity. Default=False.")

    # # Directories
    # parser.add_argument("-r", "--project-root", default=None,
    #                     dest="project_root", type=str,
    #                     help="""Directory in which the project will reside.
    #                     Default is specified in the pipeline config file.""")
    # parser.add_argument("--html-root", default=None,
    #                     dest="html_root", type=str,
    #                     help="""public_html directory in which bigwig files for the project will reside.
    #                     Default is specified in the pipeline config file.""")
    # parser.add_argument("--url-root", default=None,
    #                     dest="url_root", type=str,
    #                     help="""Url mapping to public_html directory where bigwig files for the project will be accessed.
    #                     Default is specified in the pipeline config file.""")

    # Slurm-related
    parser.add_argument("-c", "--cpus", default=None, dest="cpus",
                        help="Number of CPUs to use. Default is specified in the pipeline config file.", type=int)
    parser.add_argument("-m", "--mem-per-cpu", default=4000, dest="mem",
                        help="Memory per CPU to use. Default is specified in the pipeline config file.", type=int)
    parser.add_argument("-q", "--queue", default="shortq", dest="queue",
                        choices=["develop", "shortq", "mediumq", "longq"],
                        help="Queue to submit jobs to. Default is specified in the pipeline config file.", type=str)
    parser.add_argument("-t", "--time", default="10:00:00", dest="time",
                        help="Maximum time for jobs to run. Default is specified in the pipeline config file.", type=str)
    parser.add_argument("-u", "--user-mail", default="mail@example.com", dest="user_mail",
                        help="User email.", type=str)

    return parser


def preprocessArgParser(subparser):
    """
    subparser for preprocess
    """

    # preprocess
    preprocess_subparser = subparser.add_parser("preprocess")
    preprocess_subparser.add_argument("-s", "--stage", default="all", dest="stage",
                                      choices=["all", "bam2fastq", "fastqc", "trim", "mapping",
                                               "shiftreads", "markduplicates", "removeduplicates",
                                               "indexbam", "maketracks", "coverage"],
                                      help="Run only these stages. Default=all.", type=str)
    preprocess_subparser.add_argument("--trimmer", default="skewer", choices=["trimmomatic", "skewer"],
                                      dest="trimmer", help="Trimmer to use. Default=skewer.", type=str)
    preprocess_subparser.add_argument("-i", "--max-insert-size", default=2000,
                                      dest="maxinsert",
                                      help="Maximum allowed insert size allowed for paired end mates. Default=2000.",
                                      type=int)
    preprocess_subparser.add_argument("--window-size", default=1000, dest="windowsize",
                                      help="Window size used for genome-wide correlations. Default=1000.",
                                      type=int)
    return subparser


def analyzeArgParser(subparser):
    """
    subparser for stats.
    """
    # analyse
    analyse_subparser = subparser.add_parser("analyse")
    analyse_subparser.add_argument("-s", "--stage", default="all", dest="stage",
                                   choices=["all", "qc", "callpeaks", "findmotifs", "centerpeaks",
                                            "annotatepeaks", "peakanalysis", "tssanalysis", "footprints", "frip"],
                                   help="Run only these stages. Default=all.", type=str)
    analyse_subparser.add_argument("--peak-caller", default="macs2", choices=["macs2", "spp"],
                                   dest="peak_caller", help="Peak caller to use. Default=macs2.", type=str)
    analyse_subparser.add_argument("--peak-window-width", default=2000,
                                   dest="peak_window_width",
                                   help="Width of window around peak motifs. Default=2000.",
                                   type=int)
    return subparser


def compareArgParser(subparser):
    """
    subparser for compare.
    """
    # compare
    compare_subparser = subparser.add_parser("compare")
    compare_subparser.add_argument("-s", "--stage", default="all", dest="stage",
                                   choices=["all", "diffbind", "correlations"],
                                   help="Run only these stages. Default=all.", type=str)
    compare_subparser.add_argument("--duplicates", dest="duplicates", action="store_true",
                                   help="Allow duplicates in coorelation analysis. Default=False.")
    compare_subparser.add_argument("--genome-window-width", default=1000,
                                   dest="genome_window_width",
                                   help="Width of window to make genome-wide correlations. Default=1000.",
                                   type=int)

    return subparser


def createLogger(args, prj):
    """
    Creates a logger for the run.
    """

    global logger
    logger = logging.getLogger(__name__)
    levels = {
        "DEBUG": logging.DEBUG,
        "INFO": logging.INFO,
        "WARNING": logging.WARNING,
        "ERROR": logging.ERROR,
        "CRITICAL": logging.CRITICAL
    }
    logger.setLevel(levels[args.log_level])

    # create a file handler
    # (for now in current working dir, in the end copy log to projectDir)
    handler = logging.FileHandler(os.path.join(os.getcwd(), args.project_name + ".log"))
    handler.setLevel(logging.INFO)
    # format logger
    formatter = logging.Formatter(fmt='%(levelname)s: %(asctime)s - %(message)s', datefmt='%Y/%m/%d %H:%M:%S')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    # create a stout handler
    stdout = logging.StreamHandler(sys.stdout)
    stdout.setLevel(logging.ERROR)
    formatter = logging.Formatter(fmt='%(levelname)s: %(asctime)s - %(message)s', datefmt='%Y/%m/%d %H:%M:%S')
    stdout.setFormatter(formatter)
    logger.addHandler(stdout)

    prj.logger = logger


def sampleLoop(args, prj):
    """
    This loops through all samples and submits a job to the pipeline.

    :param args: Arguments from ArgumentParser.
    :type args: ArgumentParser
    :param prj: `Project` object.
    :type prj: Project
    """

    prj.logger.info("Starting sample preprocessing.")

    # start pipeline
    runName = "_".join([prj.name, "preprocess", time.strftime("%Y%m%d-%H%M%S")])

    # add track headers to track hubs
    for genome in pd.DataFrame(prj.samples)["genome"].unique():
        with open(os.path.join(prj.dirs.html, "trackHub_{0}.txt".format(genome)), "w") as handle:
            handle.write("browser position chr21:28,049,584-38,023,583\n")

    # Loop through samples, submit to corresponding job (preprocess, analyse)
    for sample in prj.samples:
        # get jobname
        jobName = "_".join([runName, sample.sampleName])

        # assemble command
        # slurm header
        jobCode = tk.slurmHeader(
            jobName=jobName,
            output=os.path.join(prj.dirs.logs, jobName + ".slurm.log"),
            queue=args.queue,
            time=args.time,
            cpusPerTask=args.cpus,
            memPerCpu=args.mem,
            userMail=args.user_mail
        )

        if args.command == "preprocess":
            # save pickle with all objects
            samplePickle = pickle.dump((prj, sample, args))
        elif args.command == "analyse":
            # Skip samples without paired control
            if not hasattr(sample, "controlSampleName"):
                continue
            if type(sample.controlSampleName) != str:
                continue

            # Assign the sample with that name to ctrl
            ctrl = [s for s in prj.samples if s.sampleName == sample.controlSampleName]
            # if there is only one record, use that as control
            if len(ctrl) == 1:
                ctrl = ctrl[0]
            else:
                prj.logger.error("Provided control sample name is not unambigous: %s" % ctrl)
                continue

            # save pickle with all objects (this time, 2nd element is a tuple!)
            samplePickle = pickle.dump((prj, (sample, ctrl), args))

        # Actual call to pipeline
        jobCode += "chipseq-pipeline {0}".format(samplePickle)

        # Slurm footer
        jobCode += tk.slurmFooter()

        # Save code as executable
        jobFile = os.path.join(prj.dirs.executables, jobName + ".sh")
        with open(jobFile, 'w') as handle:
            handle.write(textwrap.dedent(jobCode))

        # Submit to slurm
        if not args.dry_run:
            prj.logger.info("Submitting job to slurm: %s" % jobName)
            status = tk.slurmSubmitJob(jobFile)

            if status != 0:
                prj.logger.error("Slurm job '%s' not successfull" % jobFile)
                sys.exit(1)
            prj.logger.debug("Project '%s'submission finished successfully." % args.project_name)

        # Create link to sample in trackHub
        tk.linkToTrackHub(
            trackHubURL=os.path.join(prj.dirs.html, "trackHub_{0}.txt".format(sample.genome)),
            fileName=os.path.join(prj.dirs.root, "ucsc_tracks_{0}.html".format(sample.genome)),
            genome=sample.genome
        )

    # write original annotation sheet to project folder
    # add field for manual sample-control pairing
    prj.sheet.df.controlSampleName = None
    prj.sheet.to_csv(os.path.join(prj.dirs.root, prj.name + ".annotation_sheet.csv"))

    prj.logger.debug("Finished preprocessing")


def readStats(args, prj):
    """
    Given an annotation sheet with replicates, gets number of reads mapped, duplicates, etc...
    """

    prj.logger.info("Starting sample read stats.")

    cols = ["readCount", "unpaired", "unaligned", "unique", "multiple", "alignmentRate"]

    for sample in prj.samples:
        # Get alignment stats
        try:
            sample.alnStats = parseBowtieStats(sample.alnRates)
        except:
            prj.logger.warn("Record with alignment rates is empty or not found for sample %s" % sample.sampleName)
            pass

        # Get duplicate stats
        try:
            dups = pd.read_csv(sample.dupsMetrics, sep="\t", comment="#", header=0)
            # drop if empty
            dups.dropna(thresh=len(dups.columns) - 1, inplace=True)

            # Add values to sample sheet
            for col in range(len(cols)):
                sample.append(dups.drop(["LIBRARY"], axis=1).ix[0][col])
        except:
            prj.logger.warn("Record with duplicates is empty or not found for sample %s" % sample.sampleName)
            pass

        # Get NSC and RSC
        qcFile = os.path.join(prj.dirs.results, "sample_QC.tsv")
        qc = parseQC(sample.sampleName, qcFile)
        sample["NSC"] = qc["NSC"]
        sample["RSC"] = qc["RSC"]
        sample["qualityTag"] = qc["qualityTag"]

        # Count peak number (if peaks exist)
        if hasattr(sample, "peaks"):
            # and if sample has peaks
            if str(sample.peaks) != "nan":
                proc = subprocess.Popen(["wc", "-l", sample.peaks], stdout=subprocess.PIPE)
                out, err = proc.communicate()
                sample["peakNumber"] = re.sub("\D.*", "", out)

        # Get FRiP from file (if exists) and add to sheet
        if hasattr(sample, "peaks"):
            # if sample has peaks
            if str(sample.peaks) != "nan":
                try:
                    with open(sample.frip, "r") as handle:
                        content = handle.readlines()

                    readsInPeaks = int(re.sub("\D", "", content[0]))
                    mappedReads = sample.readCount - sample.unaligned

                    sample["FRiP"] = readsInPeaks / mappedReads
                except:
                    prj.logger.warn("Record with FRiP value is empty or not found for sample %s" % sample.sampleName)
                    pass

    samples = pd.DataFrame(prj.samples)
    # convert duplicate rate to percentage
    samples.loc[:, 'percentDuplication'] = samples['percentDuplication'] * 100

    # write annotation sheet with biological replicates
    samples.to_csv(prj.sampleStats, index=False)

    prj.logger.debug("Finished getting read statistics.")


def compare(args, prj):
    raise NotImplementedError


def getReadType(bamFile, n=10):
    """
    Gets the read type (single, paired) and length of bam file.
    Returns tuple of (readType=string, readLength=int).
    """
    from collections import Counter
    p = subprocess.Popen(['samtools', 'view', bamFile], stdout=subprocess.PIPE)

    # Count paired alignments
    paired = 0
    readLength = Counter()
    while n > 0:
        line = p.stdout.next().split("\t")
        flag = int(line[1])
        readLength[len(line[9])] += 1
        if 1 & flag:  # check decimal flag contains 1 (paired)
            paired += 1
        n -= 1
    p.kill()

    # Get most abundant read readLength
    readLength = sorted(readLength)[-1]

    # If at least half is paired, return True
    if paired > (n / 2):
        return ("PE", readLength)
    else:
        return ("SE", readLength)


def parseBowtieStats(statsFile):
    """
    Parses Bowtie2 stats file, returns series with values.
    """
    stats = pd.Series(index=["readCount", "unpaired", "unaligned", "unique", "multiple", "alignmentRate"])

    with open(statsFile) as handle:
        content = handle.readlines()  # list of strings per line

    # total reads
    try:
        line = [i for i in range(len(content)) if " reads; of these:" in content[i]][0]
        stats["readCount"] = re.sub("\D.*", "", content[line])
        if 7 > len(content) > 2:
            line = [i for i in range(len(content)) if "were unpaired; of these:" in content[i]][0]
            stats["unpaired"] = re.sub("\D", "", re.sub("\(.*", "", content[line]))
        else:
            line = [i for i in range(len(content)) if "were paired; of these:" in content[i]][0]
            stats["unpaired"] = stats["readCount"] - int(re.sub("\D", "", re.sub("\(.*", "", content[line])))
        line = [i for i in range(len(content)) if "aligned 0 times" in content[i]][0]
        stats["unaligned"] = re.sub("\D", "", re.sub("\(.*", "", content[line]))
        line = [i for i in range(len(content)) if "aligned exactly 1 time" in content[i]][0]
        stats["unique"] = re.sub("\D", "", re.sub("\(.*", "", content[line]))
        line = [i for i in range(len(content)) if "aligned >1 times" in content[i]][0]
        stats["multiple"] = re.sub("\D", "", re.sub("\(.*", "", content[line]))
        line = [i for i in range(len(content)) if "overall alignment rate" in content[i]][0]
        stats["alignmentRate"] = re.sub("\D.*", "", content[line])
    except IndexError:
        pass
    return stats


def parseQC(sampleName, qcFile):
    """
    Parses QC table produced by phantompeakqualtools (spp) and adds to sample annotation sheet
    sample quality metrics for each sample.
    """
    series = pd.Series(index=["NSC", "RSC", "qualityTag"])
    try:
        with open(qcFile) as handle:
            content = handle.readlines()  # list of strings per line
    except:
        return series

    try:
        line = [i for i in range(len(content)) if str(sampleName) in content[i]][0]
        attrs = content[line].strip().split("\t")
        series["NSC"] = attrs[-3]
        series["RSC"] = attrs[-2]
        series["qualityTag"] = attrs[-1]
    except IndexError:
        pass
    return series


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("Program canceled by user!")
        sys.exit(1)
