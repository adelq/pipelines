#!/usr/bin/env python

"""
ChIP-seq pipeline
"""

from argparse import ArgumentParser
import os
import sys
import logging
from pipelines import Project
from pipelines import toolkit as tk
import pandas as pd
import time
import re
import textwrap
import shutil
import subprocess

# TODO: solve pandas chained assignments
pd.options.mode.chained_assignment = None

__author__ = "Andre Rendeiro"
__copyright__ = "Copyright 2014, Andre Rendeiro"
__credits__ = []
__license__ = "GPL2"
__version__ = "0.1"
__maintainer__ = "Andre Rendeiro"
__email__ = "arendeiro@cemm.oeaw.ac.at"
__status__ = "Development"


def main():
    # Parse command-line arguments
    parser = ArgumentParser(description="ChIP-seq pipeline.")

    # Subparsers
    subparser = parser.add_subparsers(title="sub-command", dest="command")
    subparser = mainArgParser(subparser)
    subparser = preprocessArgParser(subparser)
    subparser = analyzeArgParser(subparser)
    subparser = compareArgParser(subparser)

    # Parse
    args = parser.parse_args()

    # Logging
    logger = createLogger()

    # Start project
    prj = Project(args.project_name)
    prj.addSampleSheet(args.csv)

    # Start main function
    if args.command == "preprocess":
        preprocess(args, prj)
    elif args.command == "stats":
        readStats(args, prj)
    elif args.command == "analyse":
        analyse(args, prj)
    elif args.command == "compare":
        compare(args, prj)

    # Exit
    logger.info("Finished and exiting.")

    # Copy log to projectDir
    shutil.copy2(
        os.path.join(os.getcwd(), prj.name + ".log"),
        os.path.join(prj.dirs.runs, prj.name + ".log")
    )

    sys.exit(1)


def mainArgParser(parser):
    """
    Global options for pipeline.
    """
    # Project
    parser.add_argument(dest="project_name", help="Project name.", type=str)
    parser.add_argument(dest="csv", help="CSV file with sample annotation.", type=str)  # improvement: check project dirs for csv

    # Behaviour
    parser.add_argument("-k", "--keep-tmp", dest="keep_tmp", action="store_true",
                        help="Keep intermediary files. If not it will only preserve final files. Default=False.")
    parser.add_argument("-l", "--log-level", default="INFO", dest="log_level",
                        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                        help="Logging level. Default=INFO.", type=str)
    parser.add_argument("--dry-run", dest="dry_run", action="store_true",
                        help="Dry run. Assemble commands, but do not submit jobs to slurm. Default=False.")
    parser.add_argument("--no-checks", dest="checks", action="store_false",
                        help="Don't check file existence and integrity. Default=False.")

    # Directories
    # parser.add_argument("-r", "--project-root", default=None,
    #                     dest="project_root", type=str,
    #                     help="""Directory in which the project will reside.
    #                     Default is specified in the pipeline config file.""")
    # parser.add_argument("--html-root", default=None,
    #                     dest="html_root", type=str,
    #                     help="""public_html directory in which bigwig files for the project will reside.
    #                     Default is specified in the pipeline config file.""")
    # parser.add_argument("--url-root", default=None,
    #                     dest="url_root", type=str,
    #                     help="""Url mapping to public_html directory where bigwig files for the project will be accessed.
    #                     Default is specified in the pipeline config file.""")

    # Slurm-related
    parser.add_argument("-c", "--cpus", default=None, dest="cpus",
                        help="Number of CPUs to use. Default is specified in the pipeline config file.", type=int)
    parser.add_argument("-m", "--mem-per-cpu", default=4000, dest="mem",
                        help="Memory per CPU to use. Default is specified in the pipeline config file.", type=int)
    parser.add_argument("-q", "--queue", default="shortq", dest="queue",
                        choices=["develop", "shortq", "mediumq", "longq"],
                        help="Queue to submit jobs to. Default is specified in the pipeline config file.", type=str)
    parser.add_argument("-t", "--time", default="10:00:00", dest="time",
                        help="Maximum time for jobs to run. Default is specified in the pipeline config file.", type=str)

    return parser


def preprocessArgParser(subparser):
    """
    subparser for preprocess
    """

    # preprocess
    preprocess_subparser = subparser.add_parser("preprocess")
    preprocess_subparser.add_argument("-s", "--stage", default="all", dest="stage",
                                      choices=["all", "bam2fastq", "fastqc", "trim", "mapping",
                                               "shiftreads", "markduplicates", "removeduplicates",
                                               "indexbam", "maketracks", "coverage"],
                                      help="Run only these stages. Default=all.", type=str)
    preprocess_subparser.add_argument("--trimmer", default="skewer", choices=["trimmomatic", "skewer"],
                                      dest="trimmer", help="Trimmer to use. Default=skewer.", type=str)
    preprocess_subparser.add_argument("-i", "--max-insert-size", default=2000,
                                      dest="maxinsert",
                                      help="Maximum allowed insert size allowed for paired end mates. Default=2000.",
                                      type=int)
    preprocess_subparser.add_argument("--window-size", default=1000, dest="windowsize",
                                      help="Window size used for genome-wide correlations. Default=1000.",
                                      type=int)
    return subparser


def analyzeArgParser(subparser):
    """
    subparser for stats.
    """
    # analyse
    analyse_subparser = subparser.add_parser("analyse")
    analyse_subparser.add_argument("-s", "--stage", default="all", dest="stage",
                                   choices=["all", "qc", "callpeaks", "findmotifs", "centerpeaks",
                                            "annotatepeaks", "peakanalysis", "tssanalysis", "footprints", "frip"],
                                   help="Run only these stages. Default=all.", type=str)
    analyse_subparser.add_argument("--peak-caller", default="macs2", choices=["macs2", "spp"],
                                   dest="peak_caller", help="Peak caller to use. Default=macs2.", type=str)
    analyse_subparser.add_argument("--peak-window-width", default=2000,
                                   dest="peak_window_width",
                                   help="Width of window around peak motifs. Default=2000.",
                                   type=int)
    return subparser


def compareArgParser(subparser):
    """
    subparser for compare.
    """
    # compare
    compare_subparser = subparser.add_parser("compare")
    compare_subparser.add_argument("-s", "--stage", default="all", dest="stage",
                                   choices=["all", "diffbind", "correlations"],
                                   help="Run only these stages. Default=all.", type=str)
    compare_subparser.add_argument("--duplicates", dest="duplicates", action="store_true",
                                   help="Allow duplicates in coorelation analysis. Default=False.")
    compare_subparser.add_argument("--genome-window-width", default=1000,
                                   dest="genome_window_width",
                                   help="Width of window to make genome-wide correlations. Default=1000.",
                                   type=int)

    return subparser


def createLogger(args, prj):
    """
    Creates a logger for the run.
    """

    global logger
    logger = logging.getLogger(__name__)
    levels = {
        "DEBUG": logging.DEBUG,
        "INFO": logging.INFO,
        "WARNING": logging.WARNING,
        "ERROR": logging.ERROR,
        "CRITICAL": logging.CRITICAL
    }
    logger.setLevel(levels[args.log_level])

    # create a file handler
    # (for now in current working dir, in the end copy log to projectDir)
    handler = logging.FileHandler(os.path.join(os.getcwd(), args.project_name + ".log"))
    handler.setLevel(logging.INFO)
    # format logger
    formatter = logging.Formatter(fmt='%(levelname)s: %(asctime)s - %(message)s', datefmt='%Y/%m/%d %H:%M:%S')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    # create a stout handler
    stdout = logging.StreamHandler(sys.stdout)
    stdout.setLevel(logging.ERROR)
    formatter = logging.Formatter(fmt='%(levelname)s: %(asctime)s - %(message)s', datefmt='%Y/%m/%d %H:%M:%S')
    stdout.setFormatter(formatter)
    logger.addHandler(stdout)

    prj.logger = logger


def preprocess(args, prj):
    """
    This takes unmapped Bam files and makes trimmed, aligned, duplicate marked
    and removed, indexed (and shifted if necessary) Bam files
    along with a UCSC browser track.
    """

    prj.logger.info("Starting sample preprocessing.")

    # start pipeline
    runName = "_".join([prj.name, "preprocess", time.strftime("%Y%m%d-%H%M%S")])

    # add track headers to track hubs
    for genome in pd.DataFrame(prj.samples)["genome"].unique():
        with open(os.path.join(prj.dirs.htmlDir, "trackHub_{0}.txt".format(genome)), "w") as handle:
            handle.write("browser position chr21:28,049,584-38,023,583\n")

    # Preprocess samples
    for sample in prj.samples:
        # get jobname
        jobName = "_".join([runName, sample.sampleName])

        # keep track of temporary files
        tempFiles = list()

        # assemble commands
        # get job header
        jobCode = tk.slurmHeader(
            jobName=jobName,
            output=os.path.join(prj.dirs.logs, jobName + ".slurm.log"),
            queue=args.queue,
            time=args.time,
            cpusPerTask=args.cpus,
            memPerCpu=args.mem,
            userMail=args.user_mail
        )
        if args.stage in ["all"]:
            # if more than one technical replicate, merge bams
            if type(sample.unmappedBam) == list:
                jobCode += tk.mergeBams(
                    inputBams=sample.unmappedBam,  # this is a list of sample paths
                    outputBam=os.path.join(prj.dirs.raw, sample.sampleName + ".bam")
                )
                # replace list with path to merged unmapped bam
                sample.unmappedBam = os.path.join(prj.dirs.raw, sample.sampleName + ".bam")
        if args.stage in ["all", "fastqc"]:
            # TODO:
            # Fastqc should be independent from this job but since there's no option in fastqc to specify
            # the sample name, I'll for now run it on the already renamed fastq file produced before,
            # which requires fastqc to run in the same job as the rest :S
            jobCode += tk.fastqc(
                inputBam=sample.unmappedBam,
                outputDir=prj.dirs.fastqc
            )
        # convert bam to fastq
        if args.stage in ["all", "bam2fastq"]:
            if not sample.paired:
                jobCode += tk.bam2fastq(
                    inputBam=sample.unmappedBam,
                    outputFastq=sample.fastq
                )
                tempFiles.append(sample.fastq)
            else:
                jobCode += tk.bam2fastq(
                    inputBam=sample.unmappedBam,
                    outputFastq=sample.fastq1,
                    outputFastq2=sample.fastq2,
                    unpairedFastq=sample.fastqUnpaired
                )
                tempFiles.append(sample.fastq1)
                tempFiles.append(sample.fastq2)
                tempFiles.append(sample.fastqUnpaired)

        if args.stage in ["all", "trim"]:
            # TODO:
            # Change absolute path to something usable by everyone or to an option.
            if args.trimmer == "trimmomatic":
                jobCode += tk.trimmomatic(
                    inputFastq1=sample.fastq1 if sample.paired else sample.fastq,
                    inputFastq2=sample.fastq2 if sample.paired else None,
                    outputFastq1=sample.trimmed1 if sample.paired else sample.trimmed,
                    outputFastq1unpaired=sample.trimmed1Unpaired if sample.paired else None,
                    outputFastq2=sample.trimmed2 if sample.paired else None,
                    outputFastq2unpaired=sample.trimmed2Unpaired if sample.paired else None,
                    cpus=args.cpus,
                    adapters=prj.config["adapters"],
                    log=sample.trimlog
                )
                if not sample.paired:
                    tempFiles.append(sample.fastq1)
                else:
                    tempFiles.append(sample.trimmed1)
                    tempFiles.append(sample.trimmed1Unpaired)
                    tempFiles.append(sample.trimmed2)
                    tempFiles.append(sample.trimmed2Unpaired)

            if args.trimmer == "skewer":
                jobCode += tk.skewer(
                    inputFastq1=sample.fastq1 if sample.paired else sample.fastq,
                    inputFastq2=sample.fastq2 if sample.paired else None,
                    outputPrefix=os.path.join(prj.dirs.fastq, sample.sampleName),
                    cpus=args.cpus,
                    adapters=prj.config["adapters"]
                )
                # move files to have common name
                if not sample.paired:
                    jobCode += tk.moveFile(
                        os.path.join(prj.dirs.fastq, sample.sampleName + "-trimmed.fastq"),
                        sample.trimmed
                    )
                    tempFiles.append(sample.trimmed)
                else:
                    jobCode += tk.moveFile(
                        os.path.join(prj.dirs.fastq, sample.sampleName + "-trimmed-pair1.fastq"),
                        sample.trimmed1
                    )
                    jobCode += tk.moveFile(
                        os.path.join(prj.dirs.fastq, sample.sampleName + "-trimmed-pair2.fastq"),
                        sample.trimmed2
                    )
                    tempFiles.append(sample.trimmed1)
                    tempFiles.append(sample.trimmed2)
                # move log to results dir
                jobCode += tk.moveFile(
                    os.path.join(prj.dirs.fastq, sample.sampleName + "-trimmed.log"),
                    sample.trimlog
                )

        if args.stage in ["all", "mapping"]:
            jobCode += tk.bowtie2Map(
                inputFastq1=sample.trimmed1 if sample.paired else sample.trimmed,
                inputFastq2=sample.trimmed1 if sample.paired else None,
                outputBam=sample.mapped,
                log=sample.alnRates,
                metrics=sample.alnMetrics,
                genomeIndex=prj.config["annotations"]["genomes"][sample.genome],
                maxInsert=args.maxinsert,
                cpus=args.cpus
            )
            tempFiles.append(sample.mapped)
        if args.stage in ["all", "markduplicates"]:
            jobCode += tk.markDuplicates(
                inputBam=sample.mapped,
                outputBam=sample.dups,
                metricsFile=sample.dupsMetrics
            )
        if args.stage in ["all", "removeduplicates"]:
            jobCode += tk.removeDuplicates(
                inputBam=sample.dups,
                outputBam=sample.nodups,
                cpus=args.cpus
            )
        if args.stage in ["all", "shiftreads"]:
            if sample.tagmented:
                jobCode += tk.shiftReads(
                    inputBam=sample.dups,
                    genome=sample.genome,
                    outputBam=sample.dupsshifted
                )
                jobCode += tk.shiftReads(
                    inputBam=sample.nodups,
                    genome=sample.genome,
                    outputBam=sample.nodupsshifted
                )
        if args.stage in ["all", "indexbam"]:
            for s in [sample.dups, sample.nodups, sample.dupsshifted, sample.nodupsshifted]:
                jobCode += tk.indexBam(inputBam=s)

        if args.stage in ["all", "maketracks"]:
            # right now tracks are only made for bams without duplicates
            jobCode += tk.bamToUCSC(
                inputBam=sample.nodups,
                outputBigWig=sample.bigwig,
                genomeSizes=prj.config["annotations"]["genomeSizes"][sample.genome],
                genome=sample.genome,
                tagmented=sample.tagmented
            )
            jobCode += tk.addTrackToHub(
                sampleName=sample.sampleName,
                trackURL=sample.trackURL,
                trackHub=os.path.join(prj.dirs.htmlDir, "trackHub_{0}.txt".format(sample.genome)),
                colour=sample.trackColour
            )
            tk.linkToTrackHub(
                trackHubURL=os.path.join(prj.dirs.htmlDir, "trackHub_{0}.txt".format(sample.genome)),
                fileName=os.path.join(prj.dirs.root, "ucsc_tracks_{0}.html".format(sample.genome)),
                genome=sample.genome
            )
        if args.stage in ["all", "coverage"]:
            jobCode += tk.genomeWideCoverage(
                inputBam=sample.nodups,
                genomeWindows=prj.config["annotations"]["genomewindows"][sample.genome],
                output=sample.coverage
            )

        # Remove intermediary files
        if args.stage == "all" and not args.keep_tmp:
            prj.logger.debug("Removing intermediary files")
            for fileName in tempFiles:
                jobCode += tk.removeFile(fileName)

        # Submit job to slurm
        # Get concatenated string with code from all modules
        jobCode += tk.slurmFooter()

        # Output file name
        jobFile = os.path.join(prj.dirs.executables, jobName + ".sh")
        with open(jobFile, 'w') as handle:
            handle.write(textwrap.dedent(jobCode))

        # Submit to slurm
        if not args.dry_run:
            prj.logger.info("Submitting job to slurm: %s" % jobName)
            status = tk.slurmSubmitJob(jobFile)

            if status != 0:
                prj.logger.error("Slurm job '%s' not successfull" % jobFile)
                sys.exit(1)
            prj.logger.debug("Project '%s'submission finished successfully." % args.project_name)

    # write original annotation sheet to project folder
    # add field for manual sample-control pairing
    prj.sheet.df.controlSampleName = None
    prj.sheet.to_csv(os.path.join(prj.dirs.root, prj.name, ".annotation_sheet.csv"))

    prj.logger.debug("Finished preprocessing")


def readStats(args, prj):
    """
    Given an annotation sheet with replicates, gets number of reads mapped, duplicates, etc...
    """

    prj.logger.info("Starting sample read stats.")

    cols = ["readCount", "unpaired", "unaligned", "unique", "multiple", "alignmentRate"]

    for sample in prj.samples:
        # Get alignment stats
        try:
            sample.alnStats = parseBowtieStats(sample.alnRates)
        except:
            prj.logger.warn("Record with alignment rates is empty or not found for sample %s" % sample.sampleName)
            pass

        # Get duplicate stats
        try:
            dups = pd.read_csv(sample.dupsMetrics, sep="\t", comment="#", header=0)
            # drop if empty
            dups.dropna(thresh=len(dups.columns) - 1, inplace=True)

            # Add values to sample sheet
            for col in range(len(cols)):
                sample.append(dups.drop(["LIBRARY"], axis=1).ix[0][col])
        except:
            prj.logger.warn("Record with duplicates is empty or not found for sample %s" % sample.sampleName)
            pass

        # Get NSC and RSC
        qcFile = os.path.join(prj.dirs.results, "sample_QC.tsv")
        qc = parseQC(sample.sampleName, qcFile)
        sample["NSC"] = qc["NSC"]
        sample["RSC"] = qc["RSC"]
        sample["qualityTag"] = qc["qualityTag"]

        # Count peak number (if peaks exist)
        if hasattr(sample, "peaks"):
            # and if sample has peaks
            if str(sample.peaks) != "nan":
                proc = subprocess.Popen(["wc", "-l", sample.peaks], stdout=subprocess.PIPE)
                out, err = proc.communicate()
                sample["peakNumber"] = re.sub("\D.*", "", out)

        # Get FRiP from file (if exists) and add to sheet
        if hasattr(sample, "peaks"):
            # if sample has peaks
            if str(sample.peaks) != "nan":
                try:
                    with open(sample.frip, "r") as handle:
                        content = handle.readlines()

                    readsInPeaks = int(re.sub("\D", "", content[0]))
                    mappedReads = sample.readCount - sample.unaligned

                    sample["FRiP"] = readsInPeaks / mappedReads
                except:
                    prj.logger.warn("Record with FRiP value is empty or not found for sample %s" % sample.sampleName)
                    pass

    samples = pd.DataFrame(prj.samples)
    # convert duplicate rate to percentage
    samples.loc[:, 'percentDuplication'] = samples['percentDuplication'] * 100

    # write annotation sheet with biological replicates
    samples.to_csv(prj.sampleStats, index=False)

    prj.logger.debug("Finished getting read statistics.")


def analyse(args, prj):
    """
    Perform ChIP-seq QC, call peaks, perform de novo motif discovery,
    plot signal around called peaks and TSSs.
    """
    prj.logger.info("Starting sample analysis.")

    # track jobs to submit
    jobs = dict()

    runName = "_".join([prj.name, "analyse", time.strftime("%Y%m%d-%H%M%S")])

    for sample in prj.samples:
        # Skip samples without paired control
        if not hasattr(sample, "controlSampleName"):
            continue
        if type(sample.controlSampleName) != str:
            continue

        # Assign the sample with that name to ctrl
        ctrl = [s for s in prj.samples if s.sampleName == sample.controlSampleName]
        # if there is only one record, use that as control
        if len(ctrl) == 1:
            ctrl = ctrl[0]
        else:
            continue

        jobName = "_".join([runName, sample.sampleName])

        # assemble commands
        jobCode = tk.slurmHeader(
            jobName=jobName,
            output=os.path.join(prj.dirs.logs, jobName + ".slurm.log"),
            queue=args.queue,
            time=args.time,
            cpusPerTask=args.cpus,
            memPerCpu=args.mem,
            userMail=args.user_mail
        )
        if args.stage in ["all", "qc"]:
            jobCode += tk.peakTools(
                inputBam=sample.nodups,
                output=os.path.join(prj.dirs.qc, "sample_QC.tsv"),
                plot=os.path.join(prj.dirs.qc, sample.sampleName + "_QC.pdf"),
                cpus=args.cpus
            )
        if args.stage in ["all", "callpeaks"]:
            if args.peak_caller == "macs2":
                # make dir for output
                if not os.path.exists(os.path.join(prj.dirs.peaks, sample.sampleName)):
                    os.makedirs(os.path.join(prj.dirs.peaks, sample.sampleName))

                # For point-source factors use default settings
                # For broad factors use broad settings
                jobCode += tk.macs2CallPeaks(
                    treatmentBam=sample.nodups,
                    controlBam=ctrl.nodups,
                    outputDir=os.path.join(prj.dirs.peaks, sample.sampleName),
                    sampleName=sample.sampleName,
                    genome=sample.genome,
                    broad=True if sample.broad else False
                )
            elif args.peak_caller == "spp":
                # For point-source factors use default settings
                # For broad factors use broad settings
                jobCode += tk.sppCallPeaks(
                    treatmentBam=sample.nodups,
                    controlBam=ctrl.nodups,
                    treatmentName=sample.sampleName,
                    controlName=ctrl.sampleName,
                    outputDir=os.path.join(prj.dirs.peaks, sample.sampleName),
                    broad=True if sample.broad else False,
                    cpus=args.cpus
                )
            elif args.peak_caller == "zinba":
                raise NotImplementedError("Calling peaks with Zinba is not yet implemented.")
                # jobCode += tk.bamToBed(
                #     inputBam=sample.nodups,
                #     outputBed=os.path.join(prj.dirs.peaks, sample.sampleName + ".bed"),
                # )
                # jobCode += tk.bamToBed(
                #     inputBam=ctrl.nodups,
                #     outputBed=os.path.join(prj.dirs.peaks, control.sampleName + ".bed"),
                # )
                # jobCode += tk.zinbaCallPeaks(
                #     treatmentBed=os.path.join(prj.dirs.peaks, sample.sampleName + ".bed"),
                #     controlBed=os.path.join(prj.dirs.peaks, control.sampleName + ".bed"),
                #     tagmented=sample.tagmented,
                #     cpus=args.cpus
                # )
        if args.stage in ["all", "findmotifs"]:
            if not sample.histone:
                # For TFs, find the "self" motif
                jobCode += tk.homerFindMotifs(
                    peakFile=sample.peaks,
                    genome=sample.genome,
                    outputDir=sample.motifsDir,
                    size="50",
                    length="8,10,12,14,16",
                    n_motifs=8
                )
                # For TFs, find co-binding motifs (broader region)
                jobCode += tk.homerFindMotifs(
                    peakFile=sample.peaks,
                    genome=sample.genome,
                    outputDir=sample.motifsDir + "_cobinders",
                    size="200",
                    length="8,10,12,14,16",
                    n_motifs=12
                )
            else:
                # For histones, use a broader region to find motifs
                jobCode += tk.homerFindMotifs(
                    peakFile=sample.peaks,
                    genome=sample.genome,
                    outputDir=sample.motifsDir,
                    size="1000",
                    length="8,10,12,14,16",
                    n_motifs=20
                )

        if args.stage in ["all", "centerpeaks"]:
            # TODO:
            # right now this assumes peaks were called with MACS2
            # figure a way of magetting the peak files withough using the peak_caller option
            # for that would imply taht option would be required when selecting this stage
            jobCode += tk.centerPeaksOnMotifs(
                peakFile=sample.peaks,
                genome=sample.genome,
                windowWidth=prj.config["options"]["peakwindowwidth"],
                motifFile=os.path.join(sample.motifsDir, "homerResults", "motif1.motif"),
                outputBed=sample.peaksMotifCentered
            )
        if args.stage in ["all", "annotatepeaks"]:
            # TODO:
            # right now this assumes peaks were called with MACS2
            # figure a way of getting the peak files withough using the peak_caller option
            # for that would imply taht option would be required when selecting this stage
            jobCode += tk.AnnotatePeaks(
                peakFile=sample.peaks,
                genome=sample.genome,
                motifFile=os.path.join(sample.motifsDir, "homerResults", "motif1.motif"),
                outputBed=sample.peaksMotifAnnotated
            )
        if args.stage in ["all", "peakanalysis"]:
            jobCode += tk.peakAnalysis(
                inputBam=sample.nodups,
                peakFile=sample.peaksMotifCentered,
                plotsDir=os.path.join(prj.dirs.results, 'plots'),
                windowWidth=prj.config["options"]["peakwindowwidth"],
                fragmentsize=1 if sample.tagmented else sample.readLength,
                genome=sample.genome,
                n_clusters=5,
                strand_specific=True,
                duplicates=True
            )
        if args.stage in ["all", "tssanalysis"]:
            jobCode += tk.tssAnalysis(
                inputBam=sample.nodups,
                tssFile=prj.config["annotations"]["tss"][sample.genome],
                plotsDir=os.path.join(prj.dirs.results, 'plots'),
                windowWidth=prj.config["options"]["peakwindowwidth"],
                fragmentsize=1 if sample.tagmented else sample.readLength,
                genome=sample.genome,
                n_clusters=5,
                strand_specific=True,
                duplicates=True
            )
        if args.stage in ["all", "frip"]:
            jobCode += tk.calculateFRiP(
                inputBam=sample.nodups,
                inputBed=sample.peaks,
                output=sample.frip
            )

        # if args.stage in ["all", "footprints"] and control and sample.tagmented:
        #     jobCode += tk.footprintAnalysis(
        #         prj.dirs.peaks, sampleName, sampleName + "_peaks.motifCentered.bed"),
        #         prj.dirs.peaks, sampleName, sampleName + "_peaks.motifAnnotated.bed")
        #     )

        # finish job
        jobCode += tk.slurmFooter()
        jobs[jobName] = jobCode

    # Submit jobs to slurm
    for jobName, jobCode in jobs.items():
        # Output file name
        jobFile = os.path.join(prj.dirs.executables, jobName + ".sh")

        with open(jobFile, 'w') as handle:
            handle.write(textwrap.dedent(jobCode))

        # Submit to slurm
        if not args.dry_run:
            prj.logger.info("Submitting jobs to slurm")
            status = tk.slurmSubmitJob(jobFile)

            if status != 0:
                prj.logger.error("Slurm job '%s' not successfull" % jobFile)
                sys.exit(1)
            prj.logger.debug("Project '%s'submission finished successfully." % args.project_name)

    # write original annotation sheet to project folder
    prj.sheet.to_csv(os.path.join(prj.dirs.root, prj.name, ".annotation_sheet.csv"))

    prj.logger.debug("Finished comparison")


def compare(args, prj):
    prj.logger.info("Starting sample comparison.")

    # start pipeline
    runName = "_".join([prj.name, "compare", time.strftime("%Y%m%d-%H%M%S")])

    samples = prj.sheet.df

    # track jobs to submit
    jobs = dict()

    # diffBind
    if args.stage in ["all", "diffbind"]:

        # Submit separate jobs for each genome
        genome_groups = samples.groupby(["genome"]).groups

        for genome in genome_groups.keys():
            # Separate comparison per IPed factor
            df = samples.ix[genome_groups[genome]]
            IP_groups = df.groupby(["ip"]).groups

            for IP in IP_groups.keys():
                if IP.upper() in ["INPUT", "IGG"]:  # skip groups with control
                    continue

                jobName = runName + "_" + "diffBind_{0}_{1}".format(genome, IP)

                # make diffBind csv file, save it
                empty = makeDiffBindSheet(
                    samples=samples,
                    df=samples.ix[IP_groups[IP]],
                    peaksDir=prj.dirs.peaks,
                    sheetFile=prj.diffBindCSV
                )
                # ^^ returns Bool for empty diffBind sheet

                if not empty:
                    # create job
                    jobCode = tk.slurmHeader(
                        jobName=jobName,
                        output=os.path.join(prj.dirs.logs, jobName + ".slurm.log"),
                        queue=args.queue,
                        time=args.time,
                        cpusPerTask=args.cpus,
                        memPerCpu=args.mem,
                        userMail=args.user_mail
                    )
                    jobCode += tk.diffBind(
                        inputCSV=prj.diffBindCSV,
                        jobName=jobName,
                        plotsDir=os.path.join(prj.dirs.results, 'plots')
                    )
                    jobCode += tk.slurmFooter()
                    jobs[jobName] = jobCode

    # Submit job for all samples together (per genome)
    if args.stage in ["all", "correlations"]:
        # Submit separate jobs for each genome
        genome_groups = samples.groupby(["genome"]).groups

        for genome in genome_groups.keys():
            jobName = runName + "_" + genome + "_sample_correlations"

            # Separate comparison per IPed factor
            df = samples.ix[genome_groups[genome]]

            jobCode = tk.slurmHeader(
                jobName=jobName,
                output=os.path.join(prj.dirs.logs, jobName + ".slurm.log"),
                queue=args.queue,
                time=args.time,
                cpusPerTask=args.cpus,
                memPerCpu=args.mem,
                userMail=args.user_mail
            )
            jobCode += tk.plotCorrelations(
                inputCoverage=[sample.coverage for sample in prj.samples],
                plotsDir=os.path.join(prj.dirs.results, 'plots')
            )
            jobCode += tk.slurmFooter()
            jobs[jobName] = jobCode

    # Submit jobs to slurm
    for jobName, jobCode in jobs.items():
        # Output file name
        jobFile = os.path.join(prj.dirs.executables, jobName + ".sh")

        with open(jobFile, 'w') as handle:
            handle.write(textwrap.dedent(jobCode))

        # Submit to slurm
        if not args.dry_run:
            prj.logger.info("Submitting jobs to slurm")
            status = tk.slurmSubmitJob(jobFile)

            if status != 0:
                prj.logger.error("Slurm job '%s' not successfull" % jobFile)
                sys.exit(1)
            prj.logger.debug("Project '%s'submission finished successfully." % args.project_name)

    prj.logger.debug("Finished comparison")


def getReadType(bamFile, n=10):
    """
    Gets the read type (single, paired) and length of bam file.
    Returns tuple of (readType=string, readLength=int).
    """
    from collections import Counter
    p = subprocess.Popen(['samtools', 'view', bamFile], stdout=subprocess.PIPE)

    # Count paired alignments
    paired = 0
    readLength = Counter()
    while n > 0:
        line = p.stdout.next().split("\t")
        flag = int(line[1])
        readLength[len(line[9])] += 1
        if 1 & flag:  # check decimal flag contains 1 (paired)
            paired += 1
        n -= 1
    p.kill()

    # Get most abundant read readLength
    readLength = sorted(readLength)[-1]

    # If at least half is paired, return True
    if paired > (n / 2):
        return ("PE", readLength)
    else:
        return ("SE", readLength)


def parseBowtieStats(statsFile):
    """
    Parses Bowtie2 stats file, returns series with values.
    """
    stats = pd.Series(index=["readCount", "unpaired", "unaligned", "unique", "multiple", "alignmentRate"])

    with open(statsFile) as handle:
        content = handle.readlines()  # list of strings per line

    # total reads
    try:
        line = [i for i in range(len(content)) if " reads; of these:" in content[i]][0]
        stats["readCount"] = re.sub("\D.*", "", content[line])
        if 7 > len(content) > 2:
            line = [i for i in range(len(content)) if "were unpaired; of these:" in content[i]][0]
            stats["unpaired"] = re.sub("\D", "", re.sub("\(.*", "", content[line]))
        else:
            line = [i for i in range(len(content)) if "were paired; of these:" in content[i]][0]
            stats["unpaired"] = stats["readCount"] - int(re.sub("\D", "", re.sub("\(.*", "", content[line])))
        line = [i for i in range(len(content)) if "aligned 0 times" in content[i]][0]
        stats["unaligned"] = re.sub("\D", "", re.sub("\(.*", "", content[line]))
        line = [i for i in range(len(content)) if "aligned exactly 1 time" in content[i]][0]
        stats["unique"] = re.sub("\D", "", re.sub("\(.*", "", content[line]))
        line = [i for i in range(len(content)) if "aligned >1 times" in content[i]][0]
        stats["multiple"] = re.sub("\D", "", re.sub("\(.*", "", content[line]))
        line = [i for i in range(len(content)) if "overall alignment rate" in content[i]][0]
        stats["alignmentRate"] = re.sub("\D.*", "", content[line])
    except IndexError:
        pass
    return stats


def parseMACSModel(modelFile):
    """
    Parses Bowtie2 stats file, returns series with values.
    """
    out = dict()

    with open(modelFile) as handle:
        content = handle.readlines()  # list of strings per line

    try:
        patt = re.compile("\((.+)\)")
        line = [i for i in range(len(content)) if "p <- c(" in content[i]][0]
        out["p"] = [float(i) for i in patt.search(content[line]).group(1).split(",")]
        line = [i for i in range(len(content)) if "m <- c(" in content[i]][0]
        out["m"] = [float(i) for i in patt.search(content[line]).group(1).split(",")]
        line = [i for i in range(len(content)) if "ycorr <- c(" in content[i]][0]
        out["ycorr"] = [float(i) for i in patt.search(content[line]).group(1).split(",")]
        line = [i for i in range(len(content)) if "xcorr <- c(" in content[i]][0]
        out["xcorr"] = [float(i) for i in patt.search(content[line]).group(1).split(",")]
        line = [i for i in range(len(content)) if "altd  <- c(" in content[i]][0]
        out["altd"] = [float(i) for i in patt.search(content[line]).group(1).split(",")]
    except IndexError:
        pass
    return out


def parseQC(sampleName, qcFile):
    """
    Parses QC table produced by phantompeakqualtools (spp) and adds to sample annotation sheet
    sample quality metrics for each sample.
    """
    series = pd.Series(index=["NSC", "RSC", "qualityTag"])
    try:
        with open(qcFile) as handle:
            content = handle.readlines()  # list of strings per line
    except:
        return series

    try:
        line = [i for i in range(len(content)) if str(sampleName) in content[i]][0]
        attrs = content[line].strip().split("\t")
        series["NSC"] = attrs[-3]
        series["RSC"] = attrs[-2]
        series["qualityTag"] = attrs[-1]
    except IndexError:
        pass
    return series


def makeDiffBindSheet(samples, df, peaksDir, sheetFile):
    """
    Prepares and saves a diffBind annotation sheet from a pandas.DataFrame annotation with standard columns.
    """
    # Exclude samples without matched control
    df = df[df["controlSampleName"].notnull()]

    # Convert numerical fields to str
    for col in ["numberCells", "technique", "treatment", "patient"]:
        df[col] = df[col].apply(str)

    # Merge some annotation fields
    df.loc[:, "condition"] = df["numberCells"] + "_" + df["technique"]
    df.loc[:, "treatment"] = df["treatment"] + "_" + df["patient"]

    # Complete rest of annotation
    df.loc[:, "ControlID"] = df['controlSampleName']
    df.loc[:, "bamControl"] = [samples[samples['sampleName'] == ctrl]["filePath"].tolist()[0] for ctrl in df['controlSampleName']]
    # TODO:
    # Change peak path according to narrow/broad
    df.loc[:, "Peaks"] = [os.path.join(peaksDir, sampleName, sampleName + "_peaks.narrowPeak") for sampleName in df["sampleName"]]
    df["PeakCaller"] = "macs"

    # Filter columns used
    df = df[["sampleName", "cellLine", "ip", "condition", "treatment",
             "biologicalReplicate", "filePath", "ControlID", "bamControl", "Peaks", "PeakCaller"]]
    # Rename columns
    df.columns = ["SampleID", "Tissue", "Factor", "Condition", "Treatment",
                  "Replicate", "bamReads", "ControlID", "bamControl", "Peaks", "PeakCaller"]

    # If resulting table is not empty, write to disk
    if not df.empty:
        # save as csv
        df.to_csv(sheetFile, index=False)  # check if format complies with DiffBind

    # Return bool of empty
    return df.empty


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("Program canceled by user!")
        sys.exit(1)
