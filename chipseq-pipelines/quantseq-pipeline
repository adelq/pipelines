#!/usr/bin/env python

"""
ChIP-seq pipeline
"""

from argparse import ArgumentParser
import os
import sys
import logging
import pandas as pd
import numpy as np
import time
import random
import re
import string
import textwrap
import shutil
import subprocess

# TODO: solve pandas chained assignments
pd.options.mode.chained_assignment = None

__author__ = "Andre Rendeiro"
__copyright__ = "Copyright 2014, Andre Rendeiro"
__credits__ = []
__license__ = "GPL3"
__version__ = "0.1"
__maintainer__ = "Andre Rendeiro"
__email__ = "arendeiro@cemm.oeaw.ac.at"
__status__ = "Development"


def main():
    # Parse command-line arguments
    parser = ArgumentParser(description="ChIP-seq pipeline.")

    # Global options
    # positional arguments
    # optional arguments
    parser.add_argument("-r", "--project-root", default="/fhgfs/groups/lab_bock/shared/projects/",
                        dest="project_root", type=str,
                        help="""Directory in which the project will reside.
                        Default=/fhgfs/groups/lab_bock/shared/projects/.""")
    parser.add_argument("--html-root", default="/fhgfs/groups/lab_bock/public_html/arendeiro/",
                        dest="html_root", type=str,
                        help="""public_html directory in which bigwig files for the project will reside.
                        Default=/fhgfs/groups/lab_bock/public_html/.""")
    parser.add_argument("--url-root", default="http://www.biomedical-sequencing.at/bocklab/arendeiro/",
                        dest="url_root", type=str,
                        help="""Url mapping to public_html directory where bigwig files for the project will be accessed.
                        Default=http://www.biomedical-sequencing.at/bocklab.""")
    parser.add_argument("--keep-tmp-files", dest="keep_tmp", action="store_true",
                        help="Keep intermediary files. If not it will only preserve final files. Default=False.")
    parser.add_argument("-c", "--cpus", default=4, dest="cpus",
                        help="Number of CPUs to use. Default=4.", type=int)
    parser.add_argument("-m", "--mem-per-cpu", default=4000, dest="mem",
                        help="Memory per CPU to use. Default=4000.", type=int)
    parser.add_argument("-q", "--queue", default="shortq", dest="queue",
                        choices=["develop", "shortq", "mediumq", "longq"],
                        help="Queue to submit jobs to. Default=shortq", type=str)
    parser.add_argument("-t", "--time", default="10:00:00", dest="time",
                        help="Maximum time for jobs to run. Default=10:00:00", type=str)
    parser.add_argument("--user-mail", default="", dest="user_mail",
                        help="User mail address. Default=<submitting user>.", type=str)
    parser.add_argument("-l", "--log-level", default="INFO", dest="log_level",
                        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                        help="Logging level. Default=INFO.", type=str)
    parser.add_argument("--dry-run", dest="dry_run", action="store_true",
                        help="Dry run. Assemble commands, but do not submit jobs to slurm. Default=False.")
    parser.add_argument("--no-checks", dest="checks", action="store_false",
                        help="Don't check file existence and integrity. Default=False.")

    # Sub commands
    subparser = parser.add_subparsers(title="sub-command", dest="command")

    # preprocess
    preprocess_subparser = subparser.add_parser("preprocess")
    preprocess_subparser.add_argument(dest="project_name", help="Project name.", type=str)
    preprocess_subparser.add_argument(dest="csv", help="CSV file with sample annotation.", type=str)
    preprocess_subparser.add_argument("-s", "--stage", default="all", dest="stage",
                                      choices=["all", "bam2fastq", "fastqc", "trim", "mapping",
                                               "shiftreads", "markduplicates", "removeduplicates",
                                               "indexbam", "maketracks", "coverage"],
                                      help="Run only these stages. Default=all.", type=str)
    preprocess_subparser.add_argument("--trimmer", default="skewer", choices=["trimmomatic", "skewer"],
                                      dest="trimmer", help="Trimmer to use. Default=skewer.", type=str)
    preprocess_subparser.add_argument("-i", "--max-insert-size", default=2000,
                                      dest="maxinsert",
                                      help="Maximum allowed insert size allowed for paired end mates. Default=2000.",
                                      type=int)
    preprocess_subparser.add_argument("--window-size", default=1000, dest="windowsize",
                                      help="Window size used for genome-wide correlations. Default=1000.",
                                      type=int)

    # Parse
    args = parser.parse_args()

    # Logging
    logger = logging.getLogger(__name__)
    levels = {
        "DEBUG": logging.DEBUG,
        "INFO": logging.INFO,
        "WARNING": logging.WARNING,
        "ERROR": logging.ERROR,
        "CRITICAL": logging.CRITICAL
    }
    logger.setLevel(levels[args.log_level])

    # create a file handler
    # (for now in current working dir, in the end copy log to projectDir)
    handler = logging.FileHandler(os.path.join(os.getcwd(), args.project_name + ".log"))
    handler.setLevel(logging.INFO)
    # format logger
    formatter = logging.Formatter(fmt='%(levelname)s: %(asctime)s - %(message)s', datefmt='%Y/%m/%d %H:%M:%S')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    # create a stout handler
    stdout = logging.StreamHandler(sys.stdout)
    stdout.setLevel(logging.ERROR)
    formatter = logging.Formatter(fmt='%(levelname)s: %(asctime)s - %(message)s', datefmt='%Y/%m/%d %H:%M:%S')
    stdout.setFormatter(formatter)
    logger.addHandler(stdout)

    # Start main function
    if args.command == "preprocess":
        logger, fr, to = preprocess(args, logger)

    # Exit
    logger.info("Finished and exiting.")

    # Copy log to projectDir
    shutil.copy2(fr, to)

    sys.exit(1)


def checkProjectDirs(args, logger):
    # Directories and paths
    # check args.project_root exists and user has write access
    args.project_root = os.path.abspath(args.project_root)
    logger.debug("Checking if %s directory exists and is writable." % args.project_root)
    if not os.access(args.project_root, os.W_OK):
        logger.error("%s does not exist, or user has no write access.\n\
Use option '-r' '--project-root' to set a non-default project root path." % args.project_root)
        sys.exit(1)

        # check args.html_root exists and user has write access
    htmlDir = os.path.abspath(args.html_root)
    logger.debug("Checking if %s directory exists and is writable." % args.project_root)
    if not os.access(htmlDir, os.W_OK):
        logger.error("%s does not exist, or user has no write access.\n\
Use option '--html-root' to set a non-default html root path." % htmlDir)
        sys.exit(1)

    # project directories
    projectDir = os.path.join(args.project_root, args.project_name)
    dataDir = os.path.join(projectDir, "data")
    resultsDir = os.path.join(projectDir, "results")

    # make relative project dirs
    dirs = [
        projectDir,
        os.path.join(projectDir, "runs"),
        dataDir,
        os.path.join(dataDir, "fastq"),
        os.path.join(dataDir, "fastqc"),
        os.path.join(dataDir, "raw"),
        os.path.join(dataDir, "RNA_counts"),
        htmlDir,
        os.path.join(htmlDir, "bigWig"),
        resultsDir,
        os.path.join(resultsDir, "plots")
    ]
    for d in dirs:
        if not os.path.exists(d):
            os.makedirs(d)

    # chmod of paths to public_html folder
    html = [
        htmlDir,
        os.path.join(args.html_root, args.project_name),
        os.path.join(args.html_root, args.project_name, "bigWig")
    ]
    for d in html:
        try:
            os.chmod(d, 0755)
        except OSError:
            logger.error("cannot change folder's mode: %s" % d)
            continue

    htmlDir = os.path.join(args.html_root, args.project_name, "bigWig")
    urlRoot = args.url_root + args.project_name + "/bigWig/"

    return (htmlDir, projectDir, dataDir, resultsDir, urlRoot)


def getReplicates(samples):
    """
    Returns new sample annotation sheet with provided samples, plus biological replicates
    (merged technical replicates) and merged biological replicates.
    samples - a pandas.DataFrame with sample info.

    """
    # ignore some fields in the annotation sheet
    varsName = ["cellLine", "numberCells", "technique", "ip", "patient", "treatment", "biologicalReplicate", "technicalReplicate", "genome"]

    # get sample names
    samples["sampleName"] = ["_".join([str(j) for j in samples.ix[i][varsName]]) for i in samples.index]

    samplesMerged = samples.copy()

    # get merged technical replicates -> biological replicates
    variables = ["cellLine", "numberCells", "technique", "ip", "patient", "treatment", "biologicalReplicate", "genome"]

    for key, values in samples.groupby(variables).groups.items():
        rep = samples.ix[values][varsName].reset_index(drop=True).ix[0]
        if len(values) > 1:
            rep["experimentName"] = np.nan
            rep["technicalReplicate"] = 0
            rep["filePath"] = samples.ix[values]["filePath"].tolist()
            rep["sampleName"] = "_".join([str(i) for i in rep[varsName]])
            # append biological replicate to sample annotation
            samplesMerged = samplesMerged.append(rep, ignore_index=True)

    # get merged biological replicates -> merged biological replicates
    variables = ["cellLine", "numberCells", "technique", "ip", "patient", "treatment", "genome"]

    for key, values in samples.groupby(variables).groups.items():
        rep = samples.ix[values][varsName].reset_index(drop=True).ix[0]
        if len(values) > 1:
            # check samples in group are from different biological replicates
            if len(samples.ix[samples.groupby(variables).groups[key]]['biologicalReplicate'].unique()) > 1:
                rep["experimentName"] = np.nan
                rep["biologicalReplicate"] = 0
                rep["technicalReplicate"] = 0
                rep["filePath"] = samples.ix[values]["filePath"].tolist()
                rep["sampleName"] = "_".join([str(i) for i in rep[varsName]])
                # append biological replicate to sample annotation
                samplesMerged = samplesMerged.append(rep, ignore_index=True)

    # replace sample name with list of sample names for only one sample (original case)
    for i in range(len(samplesMerged)):
        if type(samplesMerged["filePath"][i]) is not list:
            samplesMerged["filePath"][i] = [samplesMerged["filePath"][i]]

    return samplesMerged.sort(["sampleName"])


def preprocess(args, logger):
    """
    This takes unmapped Bam files and makes trimmed, aligned, indexed (and shifted if necessary)
    Bam files along with a UCSC browser track.
    """

    logger.info("Starting sample preprocessing.")

    logger.debug("Checking project directories exist and creating if not.")
    htmlDir, projectDir, dataDir, resultsDir, urlRoot = checkProjectDirs(args, logger)

    # Paths to static files on the cluster
    genomeFolder = "/fhgfs/groups/lab_bock/arendeiro/share/kallisto/"
    genomeIndexes = {
        "hg19": os.path.join(genomeFolder, "Homo_sapiens.GRCh38.rel79.cdna.all.ERCC.fa"),
        "mm10": os.path.join(genomeFolder, "Mus_musculus.GRCm38.rel79.cdna.all.ERCC.fa"),
        "dr7": os.path.join(genomeFolder, "Danio_rerio.Zv9.rel79.cdna.all.ERCC.fa")
    }
    adapterFasta = "/fhgfs/groups/lab_bock/shared/cm.fa"

    # Parse sample information
    args.csv = os.path.abspath(args.csv)

    # check if exists and is a file
    if not os.path.isfile(args.csv):
        logger.error("Sample annotation '%s' does not exist, or user has no read access." % args.csv)
        sys.exit(1)

    # read in
    samples = pd.read_csv(args.csv)

    # TODO:
    # Perform checks on the variables given
    # (e.g. genome in genomeIndexes)

    # Check files exist and are not empty
    for sampleFile in samples['filePath']:
        if args.checks:
            try:
                if os.stat(sampleFile).st_size == 0:
                    logger.error("Provided file %s is empty." % sampleFile)
                    sys.exit(1)
            except OSError:
                logger.error("Provided file %s does not exist." % sampleFile)
                # raise IOError("Provided file %s does not exist." % sampleFile)
                # sys.exit(1)
                continue

    # start pipeline
    projectName = string.join([args.project_name, time.strftime("%Y%m%d-%H%M%S")], sep="_")

    # Get biological replicates and merged biological replicates
    logger.debug("Checking which samples should be merged.")
    samplesMerged = getReplicates(samples.copy())
    # ^^ this is the new annotation sheet as well
    samplesMerged['readType'] = None
    samplesMerged['readLength'] = None

    # add field for manual sample pairing
    samplesMerged["controlSampleName"] = None

    # Preprocess samples
    for sample in range(len(samplesMerged)):
        # Get sample name
        sampleName = samplesMerged["sampleName"][sample]

        # get jobname
        jobName = projectName + "_" + sampleName

        # check if sample is paired end
        try:
            if args.checks:
                readType, readLength = getReadType(samplesMerged["filePath"][sample][0])
            samplesMerged['readType'][sample] = readType
            samplesMerged['readLength'][sample] = readLength

            if readType == "PE":
                PE = True
            elif readType == "None":
                PE = None
            else:
                PE = False
        except:
            continue

        # get intermediate names for files
        if len(samplesMerged["filePath"][sample]) == 1:
            unmappedBam = samplesMerged["filePath"][sample][0]
        else:
            unmappedBam = os.path.join(dataDir, "raw", sampleName + ".bam")

        bam = os.path.join(dataDir, "mapped", sampleName + ".trimmed.bowtie2")

        # keep track of temporary files
        tempFiles = list()

        # assemble commands
        # get job header
        jobCode = slurmHeader(
            jobName=jobName,
            output=os.path.join(projectDir, "runs", jobName + ".slurm.log"),
            queue=args.queue,
            time=args.time,
            cpusPerTask=args.cpus,
            memPerCpu=args.mem,
            userMail=args.user_mail
        )
        if args.stage in ["all"]:
            # if more than one technical replicate, merge bams
            if len(samplesMerged["filePath"][sample]) > 1:
                jobCode += mergeBams(
                    inputBams=samplesMerged["filePath"][sample],  # this is a list of sample paths
                    outputBam=unmappedBam
                )
        if args.stage in ["all", "fastqc"]:
            # TODO:
            # Fastqc should be independent from this job but since there's no option in fastqc to specify
            # the sample name, I'll for now run it on the already renamed fastq file produced before,
            # which requires fastqc to run in the same job as the rest :S
            jobCode += fastqc(
                inputBam=unmappedBam,
                outputDir=os.path.join(dataDir, "fastqc")
            )
        # convert bam to fastq
        if args.stage in ["all", "bam2fastq"]:
            if not PE:
                jobCode += bam2fastq(
                    inputBam=unmappedBam,
                    outputFastq=os.path.join(dataDir, "fastq", sampleName + ".fastq")
                )
                tempFiles.append(os.path.join(dataDir, "fastq", sampleName + ".fastq"))
            else:
                jobCode += bam2fastq(
                    inputBam=unmappedBam,
                    outputFastq=os.path.join(dataDir, "fastq", sampleName + ".1.fastq"),
                    outputFastq2=os.path.join(dataDir, "fastq", sampleName + ".2.fastq"),
                    unpairedFastq=os.path.join(dataDir, "fastq", sampleName + ".unpaired.fastq")
                )
                tempFiles.append(os.path.join(dataDir, "fastq", sampleName + ".1.fastq"))
                tempFiles.append(os.path.join(dataDir, "fastq", sampleName + ".2.fastq"))
                tempFiles.append(os.path.join(dataDir, "fastq", sampleName + ".unpaired.fastq"))

        if args.stage in ["all", "trim"]:
            # TODO:
            # Change absolute path to something usable by everyone or to an option.
            if args.trimmer == "skewer":
                jobCode += skewer(
                    inputFastq1=os.path.join(dataDir, "fastq", sampleName + ".fastq") if not PE else os.path.join(dataDir, "fastq", sampleName + ".1.fastq"),
                    inputFastq2=os.path.join(dataDir, "fastq", sampleName + ".2.fastq") if PE else None,
                    outputPrefix=os.path.join(dataDir, "fastq", sampleName),
                    cpus=args.cpus,
                    adapters=adapterFasta
                )
                # move files to have common name
                if not PE:
                    jobCode += moveFile(
                        os.path.join(dataDir, "fastq", sampleName + "-trimmed.fastq"),
                        os.path.join(dataDir, "fastq", sampleName + ".trimmed.fastq")
                    )
                    tempFiles.append(os.path.join(dataDir, "fastq", sampleName + ".trimmed.fastq"))
                else:
                    jobCode += moveFile(
                        os.path.join(dataDir, "fastq", sampleName + "-trimmed-pair1.fastq"),
                        os.path.join(dataDir, "fastq", sampleName + ".1.trimmed.fastq")
                    )
                    jobCode += moveFile(
                        os.path.join(dataDir, "fastq", sampleName + "-trimmed-pair2.fastq"),
                        os.path.join(dataDir, "fastq", sampleName + ".2.trimmed.fastq")
                    )
                    tempFiles.append(os.path.join(dataDir, "fastq", sampleName + ".1.trimmed.fastq"))
                    tempFiles.append(os.path.join(dataDir, "fastq", sampleName + ".2.trimmed.fastq"))
                # move log to results dir
                jobCode += moveFile(
                    os.path.join(dataDir, "fastq", sampleName + "-trimmed.log"),
                    os.path.join(resultsDir, sampleName + ".trimlog.txt")
                )

        if args.stage in ["all", "quant"]:
            if samplesMerged["genome"][sample] not in genomeIndexes:
                logger.error("Sample %s has unsuported genome index: %s" % (sampleName, samplesMerged["genome"][sample]))
                sys.exit(1)

            jobCode += makeDir(os.path.join(dataDir, "RNA_counts", sampleName))
            jobCode += kallisto(
                inputFastq1=os.path.join(dataDir, "fastq", sampleName + ".1.trimmed.fastq") if PE else os.path.join(dataDir, "fastq", sampleName + ".trimmed.fastq"),
                inputFastq2=os.path.join(dataDir, "fastq", sampleName + ".2.trimmed.fastq") if PE else None,
                outputDir=os.path.join(dataDir, "RNA_counts", sampleName),
                genomeIndex=genomeIndexes[samplesMerged["genome"][sample]],
            )
        # Remove intermediary files
        if args.stage == "all" and not args.keep_tmp:
            logger.debug("Removing intermediary files")
            for fileName in tempFiles:
                jobCode += removeFile(fileName)

        # Submit job to slurm
        # Get concatenated string with code from all modules
        jobCode += slurmFooter()

        # Output file name
        jobFile = os.path.join(projectDir, "runs", jobName + ".sh")

        with open(jobFile, 'w') as handle:
            handle.write(textwrap.dedent(jobCode))

        # Submit to slurm
        if not args.dry_run:
            logger.info("Submitting job to slurm: %s" % jobName)
            status = slurmSubmitJob(jobFile)

            if status != 0:
                logger.error("Slurm job '%s' not successfull" % jobFile)
                sys.exit(1)
            logger.debug("Project '%s'submission finished successfully." % args.project_name)

        #  Write location of bam file in merged samples annotation sheet
        samplesMerged['filePath'][sample] = bam + ".dups.bam"

    # write original annotation sheet to project folder
    samples.to_csv(os.path.join(projectDir, args.project_name + ".annotation_sheet.csv"), index=False)
    # write annotation sheet with biological replicates to project folder
    samplesMerged.to_csv(
        os.path.join(projectDir, args.project_name + ".replicates.annotation_sheet.csv"),
        index=False
    )

    logger.debug("Finished preprocessing")

    return (logger,
            os.path.join(os.getcwd(), args.project_name + ".log"),
            os.path.join(projectDir, "runs", args.project_name + ".log"))


def getReadType(bamFile, n=10):
    """
    Gets the read type (single, paired) and length of bam file.
    Returns tuple of (readType=string, readLength=int).
    """
    from collections import Counter
    p = subprocess.Popen(['samtools', 'view', bamFile], stdout=subprocess.PIPE)

    # Count paired alignments
    paired = 0
    readLength = Counter()
    while n > 0:
        line = p.stdout.next().split("\t")
        flag = int(line[1])
        readLength[len(line[9])] += 1
        if 1 & flag:  # check decimal flag contains 1 (paired)
            paired += 1
        n -= 1
    p.kill()

    # Get most abundant read readLength
    readLength = sorted(readLength)[-1]

    # If at least half is paired, return True
    if paired > (n / 2):
        return ("PE", readLength)
    else:
        return ("SE", readLength)


def slurmHeader(jobName, output, queue="shortq", ntasks=1, time="10:00:00",
                cpusPerTask=16, memPerCpu=2000, nodes=1, userMail=""):
    command = """    #!/bin/bash
    #SBATCH --partition={0}
    #SBATCH --ntasks={1}
    #SBATCH --time={2}

    #SBATCH --cpus-per-task={3}
    #SBATCH --mem-per-cpu={4}
    #SBATCH --nodes={5}

    #SBATCH --job-name={6}
    #SBATCH --output={7}

    #SBATCH --mail-type=end
    #SBATCH --mail-user={8}

    # Start running the job
    hostname
    date

    """.format(queue, ntasks, time, cpusPerTask, memPerCpu,
               nodes, jobName, output, userMail)

    return command


def slurmFooter():
    command = """
    # Job end
    date

    """

    return command


def slurmSubmitJob(jobFile):
    command = "sbatch %s" % jobFile

    return os.system(command)


def removeFile(fileName):
    command = """
    # Removing file
    rm {0}
    """.format(fileName)

    return command


def moveFile(old, new):
    command = """
    # Moving file

    mv {0} {1}
    """.format(old, new)

    return command


def makeDir(directory):
    command = """
    # Removing file

    mkdir -p {0}
    """.format(directory)

    return command


def mergeBams(inputBams, outputBam):
    command = """
    # Merging bam files from replicates
    echo "Merging bam files from replicates"

    java -Xmx4g -jar /cm/shared/apps/picard-tools/1.118/MergeSamFiles.jar \\
    USE_THREADING=TRUE \\
    {1} \\
    OUTPUT={0}
    """.format(outputBam, (" ".join(["INPUT=%s"] * len(inputBams))) % tuple(inputBams))

    return command


def fastqc(inputBam, outputDir):
    command = """
    # Measuring sample quality with Fastqc
    echo "Measuring sample quality with Fastqc"
    module load java/jdk/1.7.0_65
    module load FastQC/0.11.2

    fastqc --noextract \\
    --outdir {0} \\
    {1}

    """.format(outputDir, inputBam)

    return command


def bam2fastq(inputBam, outputFastq, outputFastq2=None, unpairedFastq=None):
    command = """
    # Convert to Fastq format
    echo "Converting to Fastq format"

    java -Xmx4g -jar /cm/shared/apps/picard-tools/1.118/SamToFastq.jar \\
    INPUT={0} \\
    """.format(inputBam)
    if outputFastq2 is None and unpairedFastq is None:
        command += """FASTQ={0}

    """.format(outputFastq)
    else:
        command += """FASTQ={0} \\
    SECOND_END_FASTQ={1} \\
    UNPAIRED_FASTQ={2}

    """.format(outputFastq, outputFastq2, unpairedFastq)

    return command


def skewer(inputFastq1, outputPrefix, cpus, adapters, inputFastq2=None):

    PE = False if inputFastq2 is None else True
    mode = "pe" if PE else "any"

    command = """
    # Trimming adapters from sample
    echo "Trimming adapters from sample"

    skewer --quiet \\
    -t {0} \\
    -m {1} \\
    -x {2} \\
    -o {3} \\
    {4} """.format(cpus, mode, adapters, outputPrefix, inputFastq1)

    if inputFastq2 is not None:
        command += """\\
    {0}

    """.format(inputFastq2)

    return command


def kallisto(inputFastq1, outputDir, genomeIndex, inputFastq2=None):
    command = """
    # Quantify reads with kallisto
    echo "Quantifying reads with kallisto"

    kallisto quant \\
    --plaintext \\
    -l 180 \\
    -i {0} \\
    -o {1} \\
    """.format(genomeIndex, outputDir)
    if inputFastq2 is None:
        command += """{0}
    """.format(inputFastq1)
    else:
        command += """{0} \\
    {1}
    """.format(inputFastq1, inputFastq2)

    return command


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("Program canceled by user!")
        sys.exit(1)
